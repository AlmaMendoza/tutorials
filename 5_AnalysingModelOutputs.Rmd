---
title: "Analyzing MAgPIE model outputs"
author: "Isabelle Weindl (weindl@pik-potsdam.de)"
output:
  html_notebook: default
  html_document: default
  pdf_document: default
  word_document: default
subtitle: MAgPIE model development team (magpie@pik-potsdam.de)
---

# 1. Introduction

After having succesfully started a simualtion run, the next step is to evaulate the simulation results.
There are several ways to assess and evalute MAgPIE results. This tutorial gives an overview on different tools and options that can be used to analyse model outputs. 

# 2. R-scripts for output analysis
In the file "config/default.cfg", it is possible to indicate which R-scripts are executed for output analysis, after a model run is finished. Scripts evaluating single runs are stored in the folder **scripts/output/single**, while the folder **scripts/output/comparison** contains scripts that compare model output across several runs. The default MAgPIE configuration specifies the scripts *rds_report* (to be used in appMAgPIE; see explanations below), *validation* and *interpolation*: 

cfg$output <- c("rds_report","validation","interpolation")

Output scripts that are included in the folders **scripts/output/single** and **scripts/output/comparison** can also be executed via command window. To do so, windows users can open a command line prompt in the forked branch folder of MAgPIE by using **shift** + **right click** then selecting *open command window here* option.

In the command prompt, use the following command:
```{r, eval = FALSE}
Rscript output.R
``` 

You are now asked to choose the output mode:
1: Output for single run
2: Comparison across runs

In both cases, you can choose from the list of available model simulations, for which runs you want to conduct the model output analysis. 

In the next step, you can interactively indicate which output function you want to execute.

# 2. Automated model validation
If the validation script is executed, a standard evaluation pdf is created that validates numerous model outputs with a validation database containing historical data and projections for most outputs returned by the model, either visually or via statistical tests. A standard evaluation PDF consists of hundreds of evaluation outputs and usually has a length of around 1800 pages. By evaluating the model outputs on such a broad level rather than focusing only key outputs it allows to get a more complete picture of the corresponding simulation. 

However, comparison between model runs, i.e. between different scenarios, is rather difficult and inconvenient if the model results are scattered across different large PDF files. 

# 3. Interactive scenario analysis
To overcome this issue, we developed the interactive scenario analysis and evaluation tools appMAgPIE and appMAgPIElocal as part of the library *shinyresults* (https://github.com/pik-piam/shinyresults), which show evaluation plots for multiple scenarios including historical data and other projections based on an interactive selection of regions and variables. You can use this tool by running the following R command in the main folder of your model, which will automatically collect all runs in the output folder and visualize them: 

```{r comment=NA,eval=FALSE}
shinyresults::appMAgPIElocal()
```


# 4. MAgPIE outputs R package for MAgPIE

If you want to go beyond visual output analysis and predefined output evaluation facilited by scripts in the model folders **scripts/output/single** and **scripts/output/comparison**, you can use the functionality of the R package *magpie4* (https://github.com/pik-piam/magpie4). This library contains a list of common functions for extracting outputs from the MAgPIE model which are also the basis for the generation of thze automated validation pdf.




